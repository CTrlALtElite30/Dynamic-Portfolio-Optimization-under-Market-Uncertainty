{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed1ef1f-4688-4404-ba04-c092e436fb1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Starting training...\n",
      "Logging to E:/project\\outputs\\logs\\dqn_discrete\\DQN_1\n",
      "Eval num_timesteps=2000, episode_reward=-0.15 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.15    |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0205   |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 2000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000271 |\n",
      "|    n_updates        | 249      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | 0.19     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 963      |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total_timesteps  | 3424     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000116 |\n",
      "|    n_updates        | 605      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0664  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 4000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.48e-05 |\n",
      "|    n_updates        | 749      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0482  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 6000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.46e-05 |\n",
      "|    n_updates        | 1249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | 0.28     |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 755      |\n",
      "|    time_elapsed     | 9        |\n",
      "|    total_timesteps  | 6848     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.87e-05 |\n",
      "|    n_updates        | 1461     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-0.11 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.115   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 8000     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.21e-05 |\n",
      "|    n_updates        | 1749     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0523  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.56e-05 |\n",
      "|    n_updates        | 2249     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | 0.297    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 10272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 8.39e-05 |\n",
      "|    n_updates        | 2317     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.102   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 12000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.95e-05 |\n",
      "|    n_updates        | 2749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | 0.295    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 13696    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 3.14e-05 |\n",
      "|    n_updates        | 3173     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0178  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 14000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.53e-05 |\n",
      "|    n_updates        | 3249     |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0406  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 16000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.18e-05 |\n",
      "|    n_updates        | 3749     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 856      |\n",
      "|    ep_rew_mean      | 0.311    |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 619      |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total_timesteps  | 17120    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.67e-05 |\n",
      "|    n_updates        | 4029     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.038   |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 18000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.92e-05 |\n",
      "|    n_updates        | 4249     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-0.10 +/- 0.00\n",
      "Episode length: 182.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 182      |\n",
      "|    mean_reward      | -0.0962  |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.02     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.54e-06 |\n",
      "|    n_updates        | 4749     |\n",
      "----------------------------------\n",
      "Training done. Model saved to E:/project\\outputs\\models\\dqn_discrete\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 4, saw 5\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m results_file = os.path.join(LOG_DIR, \u001b[33m\"\u001b[39m\u001b[33mmonitor.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(results_file):\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# SB3 monitor.csv has comment lines starting with '#'\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# df contains columns like \"r\", \"l\", \"t\"\u001b[39;00m\n\u001b[32m     84\u001b[39m     timesteps = df[\u001b[33m\"\u001b[39m\u001b[33ml\u001b[39m\u001b[33m\"\u001b[39m].cumsum()   \u001b[38;5;66;03m# episode lengths → cumulative timesteps\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 3 fields in line 4, saw 5\n"
     ]
    }
   ],
   "source": [
    "# final compateble with SB3 che with output : \n",
    "# .zip file bayni che \\models\\dqn_discrete\\ ma: \n",
    "import os, sys, numpy as np, matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "PROJECT_ROOT = r\"E:/project\"\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from envs.trading_env_discrete import TradingEnvDiscrete\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"outputs\", \"datasets\", \"states_v1_single_asset.npz\")\n",
    "SAVE_DIR  = os.path.join(PROJECT_ROOT, \"outputs\", \"models\", \"dqn_discrete\")\n",
    "LOG_DIR   = os.path.join(PROJECT_ROOT, \"outputs\", \"logs\", \"dqn_discrete\")\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "data = np.load(DATA_PATH)\n",
    "X_train, X_val = data[\"X_train\"], data[\"X_val\"]\n",
    "prices = data[\"prices\"]\n",
    "prices_train = prices[:len(X_train)]\n",
    "prices_val   = prices[len(X_train):len(X_train)+len(X_val)]\n",
    "\n",
    "# Make vec envs (NO Monitor here)\n",
    "def make_train():\n",
    "    return TradingEnvDiscrete(X_train, prices_train, reward_type=\"log_return\")\n",
    "\n",
    "def make_val():\n",
    "    return TradingEnvDiscrete(X_val, prices_val, reward_type=\"log_return\")\n",
    "\n",
    "train_env = make_vec_env(make_train, n_envs=1, seed=42, vec_env_cls=DummyVecEnv)\n",
    "val_env   = make_vec_env(make_val,   n_envs=1, seed=43, vec_env_cls=DummyVecEnv)\n",
    "\n",
    "# Wrap with VecMonitor to record monitor.csv correctly\n",
    "train_env = VecMonitor(train_env, LOG_DIR)\n",
    "val_env   = VecMonitor(val_env,   LOG_DIR)\n",
    "\n",
    "# Callback\n",
    "eval_callback = EvalCallback(\n",
    "    val_env,\n",
    "    best_model_save_path=SAVE_DIR,\n",
    "    log_path=LOG_DIR,\n",
    "    eval_freq=2000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train DQN Agent\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=32,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    verbose=1,\n",
    "    tensorboard_log=LOG_DIR,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "model.save(os.path.join(SAVE_DIR, \"dqn_final\"))\n",
    "print(f\"Training done. Model saved to {SAVE_DIR}\")\n",
    "\n",
    "# Plot Training Rewards\n",
    "\n",
    "results_file = os.path.join(LOG_DIR, \"monitor.csv\")\n",
    "if os.path.exists(results_file):\n",
    "    # SB3 monitor.csv has comment lines starting with '#'\n",
    "    df = pd.read_csv(results_file, skiprows=1)\n",
    "    # df contains columns like \"r\", \"l\", \"t\"\n",
    "    timesteps = df[\"l\"].cumsum()   # episode lengths → cumulative timesteps\n",
    "    rewards = df[\"r\"]              # episode rewards\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(timesteps, rewards, label=\"Episode Reward\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"DQN Training Rewards (Discrete Env)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(LOG_DIR, \"dqn_training_rewards.png\")\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches=\"tight\")\n",
    "    print(f\"Training reward plot saved to {plot_path}\")\n",
    "else:\n",
    "    print(\"Warning: monitor.csv not found, no reward plot generated.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed3b4d-f632-4fe5-93a7-e9b9bc1db18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
